{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrawlSubredditDatausingPushshiftAPI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de1mEuQd7S7Y"
      },
      "source": [
        "Python Notebook to crawl subreddit data using the Pushshift API. </br>\n",
        "It takes the subreddit name, and the date range for which the data needs to be crawled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZPCCaI76_vF",
        "outputId": "76172b38-89bd-42a4-91c4-9ef1914b648b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "#Download and import all dependancies\n",
        "!pip install datetime #for collab to download the datetime lib\n",
        "import requests\n",
        "from datetime import datetime as dt, timezone\n",
        "from google.colab import drive\n",
        "import traceback\n",
        "import time\n",
        "import json \n",
        "import csv "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datetime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/22/a5297f3a1f92468cc737f8ce7ba6e5f245fcfafeae810ba37bd1039ea01c/DateTime-4.3-py2.py3-none-any.whl (60kB)\n",
            "\r\u001b[K     |█████▌                          | 10kB 16.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 30kB 4.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 40kB 4.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from datetime) (2018.9)\n",
            "Collecting zope.interface\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/89/1eb9dbb9e24f5e2c29ab1a88097b2f1333858aac3cd3cccc6c4c1c8ad867/zope.interface-5.1.2-cp36-cp36m-manylinux2010_x86_64.whl (236kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface->datetime) (50.3.0)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-4.3 zope.interface-5.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqslhC-F7IGl",
        "outputId": "aaf7a75a-9768-48d9-8f3c-59d8e5e6aa03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Mount Google drive, provide auth code to write/read from G drive. You don't need to mount this if running this notebook on local.\n",
        "drive.mount(\"/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lhMHGVM7eAU"
      },
      "source": [
        "# Formatted pushshiftUrl \n",
        "pushshiftApiUrl = \"https://api.pushshift.io/reddit/submission/search/?after={}&subreddit={}&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQYCSDoU-JT5"
      },
      "source": [
        "def crawlSubredditPosts(filename,subredditname,startDateTimestamp, endDateTimestamp):\n",
        "\tcount = 0\n",
        "\theadcount =0\n",
        "\tfileop = open(filename, 'w') #overwrite existing file or create file with name <filename>\n",
        "\tcsv_writer = csv.writer(fileop)\n",
        "\tprevious_epoch = startDateTimestamp\n",
        "\theaders = [\"author\",\"author_fullname\",\"created_utc\",\"domain\",\"full_link\",\"is_crosspostable\",\"link_flair_text\",\"num_comments\",\"num_crossposts\",\"over_18\",\"permalink\",\"score\",\"selftext\",\"title\",\"total_awards_received\"]\n",
        "    \n",
        "\twhile previous_epoch < endDateTimestamp:\n",
        "\t\tnew_url = pushshiftApiUrl.format(previous_epoch,subredditname)\n",
        "\t\tprint(\"Hitting :  \", new_url)\n",
        "        \n",
        "\t\tjson = requests.get(new_url, headers={'User-Agent': \"Bot downloader\"})\n",
        "\t\ttime.sleep(2) # pushshift has a SLA rate limit, if we send requests too fast it will start returning error messages\n",
        "\t\tjson_data = json.json()\n",
        "\t\tif 'data' not in json_data:\n",
        "\t\t\tbreak\n",
        "    \n",
        "\t\tobjects = json_data['data']\n",
        "\t\tif len(objects) == 0:\n",
        "\t\t\tbreak\n",
        "\t\t#print(objects)\n",
        "\t\t\n",
        "\t\tfor object in objects:\n",
        "\t\t\tprevious_epoch = object['created_utc'] - 1 #for next epoch. since we are getting data in asending order we'll get next set of values using this time-stamp\n",
        "\t\t\t#print(\"previous_epoch \",previous_epoch)\n",
        "\t\t\tcount += 1\n",
        "\t\t\t\n",
        "\t\t\tobject['created_utc']=dt.utcfromtimestamp(object['created_utc']).strftime(\"%Y-%m-%d\") #covert date to human readable format\n",
        "\t\t\t\n",
        "\t\t\t#print(object.keys())\n",
        "\t\t\tnew_obj ={} #to write values in csv in right order\n",
        "\t\t\tfor header in headers:\n",
        "\t\t\t    if header in object.keys():\n",
        "\t\t\t        new_obj[header]=object[header]\n",
        "\t\t\t    else:\n",
        "\t\t\t        new_obj[header]=\"NA\"\n",
        "\t\t\t\n",
        "\t\t\t#print(new_obj)\n",
        "\t\t\tif headcount == 0:\n",
        "\t\t\t    csv_writer.writerow(headers)\n",
        "\t\t\t    headcount+=1 #write headers only once\n",
        "\t\t\tcsv_writer.writerow(new_obj.values())\n",
        "\t\t\t\n",
        "\n",
        "\t\tprint(\"Saved {} record(s) till date {}\".format(count, dt.utcfromtimestamp(previous_epoch).strftime(\"%Y-%m-%d\")))\n",
        "\n",
        "\tfileop.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCLDYkSqX7yW",
        "outputId": "fc24d42b-26df-4ed8-d635-55522a3611db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        }
      },
      "source": [
        "startDate = dt(2020, 1, 1) #start date\n",
        "startDateTimestamp = int(startDate.replace(tzinfo=timezone.utc).timestamp())\n",
        "endDate = dt(2020, 4, 1) #end date\n",
        "endDateTimestamp = int(endDate.replace(tzinfo=timezone.utc).timestamp())\n",
        "print(\"startDateTimestamp \",startDateTimestamp)\n",
        "print(\"endDateTimestamp \",endDateTimestamp)\n",
        "\n",
        "subredditname = \"emacs\"\n",
        "#Passing the drive path, please mention expected if running the notebook in local filesys\n",
        "#Note due to the limits of the Pushshift API the function fetches 100 posts per API call, so need to filter extra posts if croseed the endDate limit due to paging of 100 posts.\n",
        "crawlSubredditPosts(\"/drive/My Drive/Colab Notebooks/{}.{}\".format(subredditname+\"_raw\", \"csv\"),subredditname,startDateTimestamp,endDateTimestamp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "startDateTimestamp  1577836800\n",
            "endDateTimestamp  1585699200\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1577836800&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 100 record(s) till date 2020-01-07\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1578407316&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 200 record(s) till date 2020-01-14\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1578982532&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 300 record(s) till date 2020-01-20\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1579546672&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 400 record(s) till date 2020-01-26\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1580053887&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 500 record(s) till date 2020-02-01\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1580553359&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 600 record(s) till date 2020-02-08\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1581164788&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 700 record(s) till date 2020-02-15\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1581741136&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 800 record(s) till date 2020-02-22\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1582366850&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 900 record(s) till date 2020-02-29\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1582968762&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 1000 record(s) till date 2020-03-06\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1583517624&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 1100 record(s) till date 2020-03-15\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1584245352&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 1200 record(s) till date 2020-03-21\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1584784458&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 1300 record(s) till date 2020-03-28\n",
            "Hitting :   https://api.pushshift.io/reddit/submission/search/?after=1585367348&subreddit=emacs&size=100&sort_type=created_utc&sort=asc&fields=author,author_fullname,created_utc,domain,full_link,is_crosspostable,link_flair_text,num_comments,num_crossposts,over_18,permalink,score,selftext,title,total_awards_received\n",
            "Saved 1400 record(s) till date 2020-04-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEMUiFis5yNu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}